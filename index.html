
<html>
    <head>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
        <title>Xiaohang Zhan</title>
        <meta content="Xiaohang Zhan, xiaohangzhan.github.io" name="keywords" />
        <style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
            border: 0pt none;
            font-family: inherit;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration:none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration:none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight:bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
            color: #FF0000;
        }

        em, i {
            font-style:italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.edu {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 0.5em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.edu div {
            padding-left: 100px;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }
        img.icon {
            margin-bottom: 0.5em;
            float: left;
            width: 75px;
            height: 75px;
        }

        span.blurb {
            font-style:italic;
            display:block;
            margin-top:0.75em;
            margin-bottom:0.5em;
        }

        pre, code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');



</script>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-66888300-1', 'auto');
    ga('send', 'pageview');

</script>

<body>
    <div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
        <div style="margin: 0px auto; width: 100%;">
            <img title="XiaohangZhan" style="float: left; padding-left: .01em; height: 140px;" src="XiaohangZhan.jpg" />
            <div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Xiaohang Zhan (詹晓航)</span><br />
                <div itemscope itemtype="https://schema.org/Person"><a itemprop="sameAs" content="https://orcid.org/0000-0003-2136-7592" href="https://orcid.org/0000-0003-2136-7592" target="orcid.widget" rel="noopener noreferrer" style="vertical-align:top;"><img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" style="width:1em;margin-right:.5em;" alt="ORCID iD icon">orcid.org/0000-0003-2136-7592</a></div>
                <span><strong>Researcher</strong></span><br />
                <span>Central Media Technology Institute, 2012 Labs, <a href='https://www.huawei.com/en/'>Huawei Technologies Co., Ltd</a></span><br />
                <span><a href="mailto:xiaohangzhan@outlook.com">email</a></span><br />
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2>About Me (<a href='https://github.com/XiaohangZhan'>GitHub</a>) (<a href='https://scholar.google.com/citations?user=QfquhDEAAAAJ'>Google scholar</a>)</h2>
            <div class="paper">
                I'm currently a researcher in 2012 Labs, Huawei. I received my Ph.D. degree in <a href='http://mmlab.ie.cuhk.edu.hk/'>CUHK Multimedia Laboratory</a> in 2020, supervised by <a href='http://personal.ie.cuhk.edu.hk/~ccloy/'>Chen Change Loy</a> and <a href='https://www.ie.cuhk.edu.hk/people/xotang.shtml'>Xiaoou Tang</a>.
                Before that, I received my B.E. degree in the <a href='http://www.svm.tsinghua.edu.cn/'>Department of Automative Engineering</a> (renamed School of Vehicle and Mobility) in <a href='http://www.tsinghua.edu.cn/'>Tsinghua University</a>, Beijing, China in 2016.
                <br>
                My research interests include unsupervised learning and computer vision from 2016, and computer graphics from 2020.
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2 id="work">Experience</h2>
            <div class="edu" id="huawei"><img class="icon" src="icons/huawei_icon.jpeg" title="huawei" />
                <div> <font size="20"><strong>Central Media Technology Institute, 2012 Labs, Huawei Technologies Co., Ltd</strong></font><br />
                    <br>
                    <font color="gray">August 2020 - Now. Researcher (Huawei Genius Youth Program, 2020).</font>
                </div>
                <div class="spanner"></div>
            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2 id="edu">Education</h2>
            <div class="edu" id="cuhk"><img class="icon" src="icons/cuhk_icon.png" title="cuhk" />
                <div> <font size="20"><strong>The Chinese University of Hong Kong</strong></font><br />
                    <br>
                    <font color="gray">August 2017 - August 2020 (1 year shortened). Ph.D. in Information Engineering.</font>
                </div>
                <div class="spanner"></div>
            </div>
            <div class="edu" id="thu"><img class="icon" src="icons/tsinghua_icon.png" title="thu" />
                <div> <font size="20"><strong>Tsinghua University</strong></font><br />
                    <br>
                    <font color="gray">August 2012 - July 2016. Bachelor.</font>
                    <br>
                    <font color="gray">Rank 3/86. With "Outstanding Graduate Award" (63 out of about 3500 graduates).</font>
                </div>
                <div class="spanner"></div>
            </div>
        </div>
    </div>




    <div style="clear: both;">
        <div class="section">
            <h2>News</h2>
            <div class="paper">
                <ul>
                    [03/07/2020] One paper was accepted to ECCV 2020 as oral.<br>
                    [16/06/2020] Self-Supervised Learning toolbox and benchmark codebase <a href='https://github.com/open-mmlab/OpenSelfSup'>OpenSelfSup</a> is released.<br>
                    [13/03/2020] Four papers were accepted to CVPR 2020, two of them were selected as oral presentations.<br>
                    [01/11/2019] Our team won the 1st places in all 4 tracks on Facebook AI Self-Supervision Challenge.<br>
                    [23/07/2019] One paper was accepted to ICCV 2019.<br>
                    [24/02/2019] Three papers were accepted to CVPR 2019.
                </ul>
            </div>
        </div>
    </div>



    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Publications</h2>

            <div> <strong>2021</strong><br />
                <div class="paper" id="arxiv"><img class="paper" src="images/orl.png" title="orl" />
                    <div> <strong>Unsupervised Object-Level Representation Learning from Scene Images</strong><br />
                        <br>
                        Jiahao Xie, <strong>Xiaohang Zhan</strong>, Ziwei Liu, Yew Soon Ong, Chen Change Loy
                        <br>
                        <em>Conference on Neural Information Processing Systems (NeurIPS), 2021</em><br />
                        <br>
                        <a href='https://arxiv.org/abs/2106.11952'>[preprint]</a>
                        <a href='https://www.mmlab-ntu.com/project/orl/'>[Project Page]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="iccv21"><img class="paper" src="images/iccv21-detco.png" title="detco" />
                    <div> <strong>DetCo: Unsupervised Contrastive Learning for Object Detection</strong><br />
                        <br>
                        Enze Xie*, Jian Ding*, Wenhai Wang, <strong>Xiaohang Zhan</strong>, Hang Xu, Zhenguo Li, Ping Luo
                        <br>
                        <em>IEEE International Conference on Computer Vision (ICCV), 2021</em><br />
                        <br>
                        <a href='https://arxiv.org/abs/2102.04803'>[preprint]</a>
                        <a href="https://github.com/xieenze/DetCo">[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

            <div> <strong>2020</strong><br />
                <div class="paper" id="arxiv"><img class="paper" src="images/bsim.png" title="bsim" />
                    <div> <strong>Beyond Single Instance Multi-view Unsupervised Representation Learning</strong><br />
                        <br>
                        Xiangxiang Chu, <strong>Xiaohang Zhan</strong>, Xiaolin Wei
                        <br>
                        <em>ArXiv Preprints</em><br />
                        <br>
                        <a href='https://arxiv.org/abs/2011.13356'>[preprint]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="arxiv"><img class="paper" src="images/interclr.png" title="interclr" />
                    <div> <strong>Delving into Inter-Image Invariance for Unsupervised Visual Representations</strong><br />
                        <br>
                        Jiahao Xie, <strong>Xiaohang Zhan</strong>, Ziwei Liu, Yew Soon Ong, Chen Change Loy
                        <br>
                        <em>ArXiv Preprints</em><br />
                        <br>
                        <a href='https://arxiv.org/abs/2008.11702'>[preprint]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="arxiv"><img class="paper" src="images/eccv20-dgp.png" title="dgp" />
                    <div> <strong>Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation</strong><br />
                        <br>
                        Xingang Pan, <strong>Xiaohang Zhan</strong>, Bo Dai, Dahua Lin, Chen Change Loy, Ping Luo
                        <br>
                        <em>European Conference on Computer Vision (ECCV), 2020 (<font color="orange"><strong>Oral</strong></font>)</em>
                        <br>
                        <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</em><br />
                        <br>
                        <a href='https://arxiv.org/abs/2003.13659'>[preprint]</a>
                        <a href='https://xingangpan.github.io/projects/DGP.html'>[Project Page]</a>
                        <a href="https://github.com/XingangPan/deep-generative-prior">[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="CVPR20"><img class="paper" src="images/cvpr20-deocc.png" title="cvpr20-deocc" />
                    <div> <strong>Self-Supervised Scene De-occlusion</strong><br />
                        <br>
                        <strong>Xiaohang Zhan</strong>, Xingang Pan, Bo Dai, Ziwei Liu, Dahua Lin, Chen Change Loy
                        <br>
                        <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020 (<font color="orange"><strong>Oral</strong></font>)</em><br />
                        <br>
                        <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhan_Self-Supervised_Scene_De-Occlusion_CVPR_2020_paper.pdf'>[PDF]</a>
                        <a href='https://arxiv.org/abs/2004.02788'>[preprint]</a>
                        <a href='projects/deocclusion/'>[Project Page]</a>
                        <a href="https://github.com/XiaohangZhan/deocclusion">[Code]</a>
                        <a href='https://www.youtube.com/watch?v=xIHCyyaB5gU'>[Demo]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="CVPR20"><img class="paper" src="images/cvpr20-odc.png" title="cvpr20-odc" />
                    <div> <strong>Online Deep Clustering for Unsupervised Representation Learning</strong><br />
                        <br>
                        <strong>Xiaohang Zhan*</strong>, Jiahao Xie*, Ziwei Liu, Yew-Soon Ong, Chen Change Loy
                        <br>
                        <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020</em><br />
                        <br>
                        <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhan_Online_Deep_Clustering_for_Unsupervised_Representation_Learning_CVPR_2020_paper.pdf'>[PDF]</a>
                        <a href='https://arxiv.org/abs/2006.10645'>[preprint]</a>
                        <a href="https://github.com/open-mmlab/OpenSelfSup">[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="CVPR20"><img class="paper" src="images/cvpr20-clustering.png" title="cvpr20-clustering" />
                    <div> <strong>Learning to Cluster Faces via Confidence and Connectivity Estimation</strong><br />
                        <br>
                        Lei Yang, Dapeng Chen, <strong>Xiaohang Zhan</strong>, Rui Zhao, Chen Change Loy, Dahua Lin
                        <br>
                        <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020</em><br />
                        <br>
                        <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_to_Cluster_Faces_via_Confidence_and_Connectivity_Estimation_CVPR_2020_paper.pdf'>[PDF]</a>
                        <a href='https://arxiv.org/abs/2004.00445'>[preprint]</a>
                        <a href='http://yanglei.me/project/ltc_v2/'>[Project Page]</a>
                        <a href="https://github.com/yl-1993/learn-to-cluster">[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="CVPR20"><img class="paper" src="images/cvpr20-ocda.png" title="cvpr20-ocda" />
                    <div> <strong>Compound Domain Adaptation in an Open World</strong><br />
                        <br>
                        Ziwei Liu*, Zhongqi Miao*, Xingang Pan, <strong>Xiaohang Zhan</strong>, Stella X. Yu, Dahua Lin, Boqing Gong
                        <br>
                        <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020 (<font color="orange"><strong>Oral</strong></font>)</em><br />
                        <br>
                        <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Open_Compound_Domain_Adaptation_CVPR_2020_paper.pdf'>[PDF]</a>
                        <a href='https://arxiv.org/abs/1909.03403'>[preprint]</a>
                        <a href='https://liuziwei7.github.io/projects/CompoundDomain.html'>[Project Page]</a>
                        <a href='https://www.youtube.com/watch?v=YcmgCCRA1qc'>[Demo]</a>
                        <a href='https://github.com/zhmiao/OpenCompoundDomainAdaptation-OCDA'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div> <strong>2019</strong><br />

                <div class="paper" id="ICCV2019"><img class="paper" src="images/iccv19w-codc.png" title="iccv19w-codc" />
                    <div> <strong>Collaborative Online Deep Clustering for Unsupervised Representation Learning</strong><br />
                        <br>
                        <strong>Xiaohang Zhan</strong>, Jiahao Xie, Ziwei Liu, Yew Soon Ong, Chen Change Loy
                        <br>
                        <em>The <font color="orange">Champion</font> of <a href="https://sites.google.com/view/fb-ssl-challenge-iccv19/home">Facebook AI Self-Supervision Challenge</a> (all tracks), in ICCV <a href="https://sites.google.com/view/extremevision/">Extreme Vision Workshop</a>, 2019</em><br />
                        <br>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="ICCV2019"><img class="paper" src="images/iccv19-sw.png" title="iccv19-sw" />
                    <div> <strong>Switchable Whitening for Deep Representation Learning</strong><br />
                        <br>
                        Xingang Pan, <strong>Xiaohang Zhan</strong>, Jianping Shi, Xiaoou Tang, Ping Luo
                        <br>
                        <em>IEEE International Conference on Computer Vision (ICCV), 2019</em><br />
                        <br>
                        <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Pan_Switchable_Whitening_for_Deep_Representation_Learning_ICCV_2019_paper.pdf">[PDF]</a>
                        <a href='https://arxiv.org/abs/1904.09739'>[preprint]</a>
                        <a href="https://github.com/XingangPan/Switchable-Whitening">[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="CVPR2019"><img class="paper" src="images/cvpr19-longtail.png" title="cvpr19-longtail" />
                    <div> <strong>Large-scale Long-Tailed Recognition in an Open World</strong><br />
                        <br>
                        Ziwei Liu*, Zhongqi Miao*, <strong>Xiaohang Zhan</strong>, Jiayun Wang, Boqing Gong, Stella X. Yu
                        <br>
                        <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019 (<font color="orange"><strong>Oral</strong></font>)</em><br />
                        <br>
                        <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Large-Scale_Long-Tailed_Recognition_in_an_Open_World_CVPR_2019_paper.pdf">[PDF]</a>
                        <a href='https://arxiv.org/abs/1904.05160'>[preprint]</a>
                        <a href='https://liuziwei7.github.io/projects/LongTail.html'>[Project Page]</a>
                        <a href='https://github.com/zhmiao/OpenLongTailRecognition-OLTR'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="CVPR2019"><img class="paper" src="images/cvpr19-gcn.png" title="cvpr19-gcn" />
                    <div style="padding-top: 2%"> <strong>Learning to Cluster Faces on an Affinity Graph</strong><br />
                        <br>
                        Lei Yang, <strong>Xiaohang Zhan</strong>, Dapeng Chen, Junjie Yan, Chen Change Loy, Dahua Lin
                        <br>
                        <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019 (<font color="orange"><strong>Oral</strong></font>)</em><br />
                        <br>
                        <a href='http://personal.ie.cuhk.edu.hk/~ccloy/files/cvpr_2019_cluster.pdf'>[PDF]</a>
                        <a href='https://arxiv.org/abs/1904.02749'>[preprint]</a>
                        <a href='http://yanglei.me/project/ltc/'>[Project Page]</a>
                        <a href='https://github.com/yl-1993/learn-to-cluster'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="CVPR2019"><img class="paper" src="images/cvpr19-cmp.png" title="cvpr19-cmp" />
                    <div style="padding-top: 3.5%"> <strong>Self-Supervised Learning via Conditional Motion Propagation</strong><br />
                        <br>
                        <strong>Xiaohang Zhan</strong>, Xingang Pan, Ziwei Liu, Dahua Lin, Chen Change Loy
                        <br>
                        <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019 </em><br />
                        <br>
                        <a href='http://personal.ie.cuhk.edu.hk/~ccloy/files/cvpr_2019_self.pdf'>[PDF]</a>
                        <a href='https://arxiv.org/abs/1903.11412'>[preprint]</a>
                        <a href='http://mmlab.ie.cuhk.edu.hk/projects/CMP/'>[Project Page]</a>
                        <a href='https://github.com/XiaohangZhan/conditional-motion-propagation/'>[Code]</a>
                        <a href='https://www.youtube.com/watch?v=6R_oJCq5qMw'>[Demo]</a>
                    </div>
                    <div class="spanner"></div>
                </div>
            </div>


            <div> <strong>2018</strong><br />
                <div class="paper" id="ECCV2018"><img class="paper" src="images/eccv18-cdp.png" title="eccv18-cdp" />
                    <div> <strong>Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition</strong><br />
                        <br>
                        <strong>Xiaohang Zhan</strong>, Ziwei Liu, Junjie Yan, Dahua Lin, Chen Change Loy
                        <br>
                        <em>European Conference on Computer Vision (ECCV), 2018</em><br />
                        <br>
                        <a href='http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2018_cdp.pdf'>[PDF]</a>
                        <a href='https://arxiv.org/abs/1809.01407'>[preprint]</a>
                        <a href='http://mmlab.ie.cuhk.edu.hk/projects/CDP/'>[Project Page]</a>
                        <a href='https://github.com/XiaohangZhan/cdp/'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

                <div class="paper" id="AAAI2018"><img class="paper" src="images/aaai18-mm.png" title="aaai18-mm" />
                    <div> <strong>Mix-and-Match Tuning for Self-Supervised Semantic Segmentation</strong><br />
                        <br>
                        <strong>Xiaohang Zhan</strong>, Ziwei Liu, Ping Luo, Xiaoou Tang, Chen Change Loy
                        <br>
                        <em>AAAI Conference on Artificial Intelligence (AAAI), 2018 (<font color="orange"><strong>Spotlight</strong></font>)</em><br />
                        <br>
                        <a href='http://personal.ie.cuhk.edu.hk/~ccloy/files/aaai_2018_mix.pdf'>[PDF]</a>
                        <a href='https://arxiv.org/abs/1712.00661'>[preprint]</a>
                        <a href='http://mmlab.ie.cuhk.edu.hk/projects/M&M/'>[Project Page]</a>
                        <a href='https://github.com/XiaohangZhan/mix-and-match'>[Code]</a>
                    </div>
                    <div class="spanner"></div>
                </div>

            </div>
        </div>
    </div>

    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Professional activities</h2>
            <div class="paper">
                <li>The major developer of OpenSelfSup, an open-source self-supervised learning toolbox (Now <a href='https://github.com/open-mmlab/mmselfsup'>mmselfsup</a>).</li>
                <li>Reviewer of T-PAMI, IJCV, CVPR, ICCV, ECCV, AAAI, BMVC.</li>
                <li><a href='https://xingangpan.github.io/projects/CULane.html'>CULane Dataset</a>: a large-scale challenging dataset for traffic lane detection.
                <ul>
                    <p><font size="5"></font></p>
                </ul>
            </div>
        </div>
    </div>

    <!---
    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Contests</h2>
            <div class="paper">
                <ul>

                </ul>
                <div class="spanner"></div>
            </div>
        </div>
    </div>
    -->

    <div style="clear: both;">
        <div class="section"><h2>Talks</h2>
            <div class="paper">
                <li>2017 @ JD: <a href=pdf/talk-jd-2017.pdf>Mix-and-Match Tuning for Self-Supervised Semantic Segmentation</a></li>
                <li>2019 @ HKSTP: <a href=pdf/talk-hkstp-2019.pdf>Self-Supervised Learning via Conditional Motion Propagation</a></li>
                <li>2020 @ SenseTime TITAN Open Classes <a href='https://www.bilibili.com/video/av883608955/'>[Video Record]</a>: <a href=pdf/talk-st-2020.pdf>Self-Supervised Learning</a></li>
                <li>2020 @ Tsinghua: <a href=pdf/talk-thu-2020.pdf>Exploiting Unlabeled Data in Computer Vision</a></li>
                <li>2020 @ Zhidongxi Open Classes <a href='https://course.zhidx.com/c/Yjk1ZjYxMmI2NjIyNTg1YmY2MDM='>[Video Record]</a>: <a href=pdf/talk-zhidongxi-2020.pdf>Understanding and Practice of OpenSelfSup: an Open-source Library for Self-Supervised Learning</a></li>
            </div>
            <div class="spanner"></div>
        </div>

    </div>


    <div style="clear: both;">
        <div class="section"><h2>Honors and Awards</h2>
            <div class="paper">
                <li>Top 100 Chinese New Stars in Artificial Intelligence by Baidu Scholar, <a href="https://mp.weixin.qq.com/s/v7ITiZXOJiDUbPlRlcqQRA">AI华人新星百强</a> (100 out of 45205).</li>
                <li>Huawei Genius Youth, 2020</li>
                <li>Winner of <a href='https://sites.google.com/view/fb-ssl-challenge-iccv19/home'>Facebook AI Self-Supervision Challenge 2019</a></li>
                <li>Hong Kong PhD Fellowship, 2017, the Research Grants Council (RGC) of Hong Kong</li>
                <li>Outstanding Graduate of Tsinghua University, 2016 (63 winners out of about 3500 graduates)</li>
                <li>Outstanding Graduate of Beijing, 2016</li>
                <li>National Scholarship, 2014, Ministry of Education of China</li>
                <li>Technological Innovation Scholarship, 2014, 2015, Tsinghua University</li>
                <li>First class Academic Scholarship, 2013, 2014, Tsinghua University</li>
                <li>Special class Integrated Excellence Scholarship, 2013, Tsinghua University</li>
                <li>“Spark” Innovative Talent Cultivation Program for Students of Tsinghua University (50 out of about 3500)</li>
            </div>
        </div>
    </div>



    <div style="clear:both;">
    <p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>
<hr>
<div id="clustrmaps-widget"></div><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=265499&w=400&t=tt&d=mEeGHmaub5GXeeTxKsShipAjJG4IG43WXN86qcXppc4&co=eeeeee&ct=ffffff&cmo=e03a3a&cmn=46cc3a"></script>

</body>
    </html>
